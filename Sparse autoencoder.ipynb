{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "http://nbviewer.ipython.org/github/mdeff/dlaudio/blob/master/auto_encoder.ipynb\n",
    "https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf\n",
    "    \n",
    "http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.optimize.minimize.html\n",
    "    \n",
    "https://chrisjmccormick.wordpress.com/2014/05/30/deep-learning-tutorial-sparse-autoencoder/\n",
    "    \n",
    "https://github.com/caglar/autoencoders/blob/master/sa.py\n",
    "    \n",
    "https://lts2.epfl.ch/blog/mdeff/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t has been demonstrated in the past that sparsity is a desirable property for feature representations:\n",
    "it increases interpretability and sometimes classification performance [8].   We can encourage an\n",
    "RBM to produce sparse features with a form of data-dependent regularization. To do this, we add a\n",
    "penalty term to the objective function which is proportional to the difference between the probability\n",
    "of activation of the hidden units and a small target value. For details, we refer to Goh et al. [5].\n",
    "\n",
    "[5]\n",
    "  Hanlin Goh, Nicolas Thome, and Matthieu Cord.  Biasing restricted boltzmann machines to manipulate latent selectivity and sparsity. In Deep Learning and Unsupervised Feature Learning Workshop — NIPS, 2010.\n",
    "\n",
    "[8]  Geoffrey E. Hinton.  A practical guide to training restricted boltzmann machines.  Technical report, University of Toronto, 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bengio\n",
    "\n",
    "Although we will not discuss this much\n",
    "here, the inspiration for a sparse representa-\n",
    "tion in Deep Learning comes from the ear-\n",
    "lier work on\n",
    "sparse coding\n",
    "(Olshausen and Field,\n",
    "1997). As discussed in Goodfellow\n",
    "et al.\n",
    "(2009)\n",
    "sparse representations may be advantageous be-\n",
    "cause they encourage representations that\n",
    "dis-\n",
    "entangle\n",
    "the underlying factors of representa-\n",
    "tion.   A sparsity-inducing penalty is also a\n",
    "way to regularize (in the sense of reducing the\n",
    "number of examples that the learner can learn\n",
    "by heart) (Ranzato\n",
    "et al.\n",
    ", 2008b), which means\n",
    "that the sparsity coefficient is likely to interact\n",
    "with the many other hyper-parameters which in-\n",
    "fluence capacity. In general, increased sparsity\n",
    "can be compensated by a larger number of hid-\n",
    "den units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bengio\n",
    "\n",
    "A\n",
    "close cousin of the L1 penalty is the Student-\n",
    "t penalt (log(1 +\n",
    "h\n",
    "2\n",
    "j\n",
    ")), originally proposed for\n",
    "sparse coding (Olshausen and Field, 1997). Sev-\n",
    "23\n",
    "because the input to the layer generally has a non-zero\n",
    "average, that when multiplied by the weights acts like a bias\n",
    "13\n",
    "eral researchers penalize the\n",
    "average\n",
    "output\n",
    " ̄\n",
    "h\n",
    "j\n",
    "(e.g. over a mini-batch), and instead of pushing\n",
    "it to 0, encourage it to approach a fixed target\n",
    "ρ\n",
    ".\n",
    "This can be done through a mean-square error\n",
    "penalty such as\n",
    "P\n",
    "j\n",
    "(\n",
    "ρ\n",
    "−\n",
    " ̄\n",
    "h\n",
    "j\n",
    ")\n",
    "2\n",
    ", or maybe more\n",
    "sensibly (because\n",
    "h\n",
    "j\n",
    "behaves like a probabil-\n",
    "ity), a Kullback-Liebler divergence with respect\n",
    "to the binomial distribution with probability\n",
    "ρ\n",
    ",\n",
    "−\n",
    "ρ\n",
    "log\n",
    " ̄\n",
    "h\n",
    "j\n",
    "−\n",
    "(1\n",
    "−\n",
    "ρ\n",
    ") log(1\n",
    "−\n",
    " ̄\n",
    "h\n",
    "j\n",
    ")+constant, e.g.,\n",
    "with\n",
    "ρ\n",
    "= 0\n",
    ".\n",
    "05, as in (Hinton, 201"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
