{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization techniques: gradient clipping, Nesterov momentum and the use of NADE for conditional density estimation.\n",
    "Hyperparameter search: learning rate (separately for the RBM and RNN parts), learning rate schedules, batch size, number of hidden units (recurrent and RBM), momentum coefficient, momentum schedule, Gibbs chain length k and early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood gradient of such\n",
    "models contains two main terms: the so-called posi-\n",
    "tive phase contribution tells the model to decrease the\n",
    "energies associated with training example\n",
    "v\n",
    "and the\n",
    "so-called negative phase contribution tells the model\n",
    "to increase the energy of all other configurations of\n",
    "(\n",
    "v\n",
    ",\n",
    "h\n",
    "), in proportion to their probability according to\n",
    "the model.\n",
    "\n",
    "\n",
    "One can envision the action\n",
    "of the CD negative phase as pushing up the energy of\n",
    "the most likely values (under the model)\n",
    "near train-\n",
    "ing examples\n",
    ". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ooks kinda feature-ish, but a bit messed up. Let’s try to add some sparsity to make hidden units represent more independent features. The basic way to to it is described by Honglak Lee, and it’s just adding ρ−mean(h0) to weights and hidden biases, where ρ is the desired sparsity target, meaning average probability that a certain unit is on. Actually it is possible to add that term only to hidden biases, and we’ll get back to it in a moment.\n",
    "\n",
    "\n",
    "At first I’d like to point out that I haven’t found literally any analysis on the matter. Energy function for Gaussian RBM is just stated (by Hinton, Lee and some other papers I’ve managed to google), and no further comments are made. That’s really disappointing especially because everyone keeps saying Gaussian RBMs are hard to train. And, by the way, this is so true.\n",
    "\n",
    "First, it doesn’t even work at all with uniformly initialized [−1m,1m] weights. To make it learn, I had to replace them with normaly distributed weights with zero mean and 0.001 standart deviation (thanks to practical guide again). Any attempt to increate the std value breaks learning like completely.\n",
    "\n",
    "Oh, and I forgot to mention the actual change: for visible units I’ve replaced the activation function with sampling from normal distribution of (hw+bvis) mean and unit variance. To be able to do that I had to rescale the input data to zero mean and unit variance (otherwise it’s also possible to learn precise variance parameters per each unit). Also I guess I can’t use “raw” hw+bvis value to collect learning statistic (as I did with Bernoulli probability), so I’m going to sample states everywhere.\n",
    "\n",
    "According to my (kinda sloppy) observations sparsity doesn’t work so good for Gaussian RBM either — adding sparsity penalty to the weights seems to push the gradient in the wrong direction maybe? Anyway, average hidden activation doesn’t change properly. I’ve followed Lee’s advice about adding sparsity penalty just to visible biases, and now it works better.\n",
    "\n",
    "\n",
    "http://rocknrollnerd.github.io/ml/2015/07/23/finally-rbms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Testar os dois codigos de sparsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    def set_learning_rate(self, epoch, de_con=1e-3):\n",
    "        return self.init_lr / (1. + de_con*epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SGD\n",
    "\n",
    "keras.optimizers.SGD(lr=0.01, momentum=0., decay=0., nesterov=False)\n",
    "\n",
    "Arguments:\n",
    "\n",
    "    lr: float >= 0. Learning rate.\n",
    "    momentum: float >= 0. Parameter updates momentum.\n",
    "    decay: float >= 0. Learning rate decay over each update.\n",
    "    nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "\n",
    "Adagrad\n",
    "\n",
    "keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\n",
    "\n",
    "It is recommended to leave the parameters of this optimizer at their default values.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "    lr: float >= 0. Learning rate.\n",
    "    epsilon: float >= 0.\n",
    "\n",
    "Adadelta\n",
    "\n",
    "keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-6)\n",
    "\n",
    "It is recommended to leave the parameters of this optimizer at their default values.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "    lr: float >= 0. Learning rate. It is recommended to leave it at the default value.\n",
    "    rho: float >= 0.\n",
    "    epsilon: float >= 0. Fuzz factor.\n",
    "\n",
    "For more info, see \"Adadelta: an adaptive learning rate method\" by Matthew Zeiler.\n",
    "RMSprop\n",
    "\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "\n",
    "It is recommended to leave the parameters of this optimizer at their default values.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "    lr: float >= 0. Learning rate.\n",
    "    rho: float >= 0.\n",
    "    epsilon: float >= 0. Fuzz factor.\n",
    "\n",
    "Adam\n",
    "\n",
    "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "Adam optimizer, proposed by Kingma and Lei Ba in Adam: A Method For Stochastic Optimization. Default parameters are those suggested in the paper.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "    lr: float >= 0. Learning rate.\n",
    "    beta_1, beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
    "    epsilon: float >= 0. Fuzz factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Available objectives\n",
    "\n",
    "    mean_squared_error / mse\n",
    "    root_mean_squared_error / rmse\n",
    "    mean_absolute_error / mae\n",
    "    mean_absolute_percentage_error / mape\n",
    "    mean_squared_logarithmic_error / msle\n",
    "    squared_hinge\n",
    "    hinge\n",
    "    binary_crossentropy: Also known as logloss.\n",
    "    categorical_crossentropy: Also known as multiclass logloss. Note: using this objective requires that your labels are binary arrays of shape (nb_samples, nb_classes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"\"\" Adadelta updates\n",
    "    Scale learning rates by a the ratio of accumulated gradients to accumulated\n",
    "    step sizes, see [1]_ and notes for further description.\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_or_grads : symbolic expression or list of expressions\n",
    "        A scalar loss expression, or a list of gradient expressions\n",
    "    params : list of shared variables\n",
    "        The variables to generate update expressions for\n",
    "    learning_rate : float or symbolic scalar\n",
    "        The learning rate controlling the size of update steps\n",
    "    rho : float or symbolic scalar\n",
    "        Squared gradient moving average decay factor\n",
    "    epsilon : float or symbolic scalar\n",
    "        Small value added for numerical stability\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict\n",
    "        A dictionary mapping each parameter to its update expression\n",
    "    Notes\n",
    "    -----\n",
    "    rho should be between 0 and 1. A value of rho close to 1 will decay the\n",
    "    moving average slowly and a value close to 0 will decay the moving average\n",
    "    fast.\n",
    "    rho = 0.95 and epsilon=1e-6 are suggested in the paper and reported to\n",
    "    work for multiple datasets (MNIST, speech).\n",
    "    In the paper, no learning rate is considered (so learning_rate=1.0).\n",
    "    Probably best to keep it at this value.\n",
    "    epsilon is important for the very first update (so the numerator does\n",
    "    not become 0).\n",
    "    Using the step size eta and a decay factor rho the learning rate is\n",
    "    calculated as:\n",
    "    .. math::\n",
    "       r_t &= \\\\rho r_{t-1} + (1-\\\\rho)*g^2\\\\\\\\\n",
    "       \\\\eta_t &= \\\\eta \\\\frac{\\\\sqrt{s_{t-1} + \\\\epsilon}}\n",
    "                             {\\sqrt{r_t + \\epsilon}}\\\\\\\\\n",
    "       s_t &= \\\\rho s_{t-1} + (1-\\\\rho)*(\\\\eta_t*g)^2\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Zeiler, M. D. (2012):\n",
    "           ADADELTA: An Adaptive Learning Rate Method.\n",
    "           arXiv Preprint arXiv:1212.5701.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARA CLIPAR OS GRADIENTES!!\n",
    "\n",
    "    def get_gradients(self, loss, params):\n",
    "        grads = K.gradients(loss, params)\n",
    "        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n",
    "            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n",
    "            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n",
    "        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n",
    "            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    Measuring training progress\n",
    "Monitoring RBM training progress is a challenging problem.  The objective function itself cannot\n",
    "be used for this purpose because it is intractable to compute.\n",
    "Annealed importance sampling\n",
    "[16] is\n",
    "sometimes used to estimate this quantity, but it is not always reliable and computationally intensive.\n",
    "A frequently used alternative is the\n",
    "one-step reconstruction error\n",
    ": the mean squared error between\n",
    "a set of data points and the reconstruction from their inferred feature representations. Unfortunately\n",
    "this measure can be very misleading, as it correlates poorly with training progress:  it strongly de-\n",
    "pends on the mixing rate of the alternating Gibbs chain that is used to draw samples from the model\n",
    "distribution (see Section 5 of Hinton [8] for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Several authors have suggested to monitor statistics and histograms of the model parameters and\n",
    "the activations of the hidden units, as well as their gradients, during training.  Bengio [1] suggests\n",
    "that visualizing these statistics and histograms can be useful to monitor training progress and to\n",
    "identify under- or overfitting.  Yosinski and Lipson [19] also note that these visualizations can be\n",
    "instrumental in identifying inadequately tuned hyperparameters and implementation bugs\n",
    "\n",
    "[19]\n",
    "  Jason Yosinski and Hod Lipson.   Visually debugging restricted boltzmann machine training with a 3d\n",
    "example.  In\n",
    "Representation Learning Workshop, 29th International Conference on Machine Learning\n",
    ",\n",
    "2012.\n",
    "[1]  Yoshua Bengio.   Practical recommendations for gradient-based training of deep architectures.\n",
    "CoRR\n",
    ",\n",
    "abs/1206.5533, 2012."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
