{
 "metadata": {
  "name": "",
  "signature": "sha256:43b8e33d56d40736ca0d24e20f907ba664e4b66d511d3e6e6ac9179f3c8dbf49"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Optimization techniques: gradient clipping, Nesterov momentum and the use of NADE for conditional density estimation.\n",
      "Hyperparameter search: learning rate (separately for the RBM and RNN parts), learning rate schedules, batch size, number of hidden units (recurrent and RBM), momentum coefficient, momentum schedule, Gibbs chain length k and early stopping."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The log-likelihood gradient of such\n",
      "models contains two main terms: the so-called posi-\n",
      "tive phase contribution tells the model to decrease the\n",
      "energies associated with training example\n",
      "v\n",
      "and the\n",
      "so-called negative phase contribution tells the model\n",
      "to increase the energy of all other configurations of\n",
      "(\n",
      "v\n",
      ",\n",
      "h\n",
      "), in proportion to their probability according to\n",
      "the model.\n",
      "\n",
      "\n",
      "One can envision the action\n",
      "of the CD negative phase as pushing up the energy of\n",
      "the most likely values (under the model)\n",
      "near train-\n",
      "ing examples\n",
      ". "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ooks kinda feature-ish, but a bit messed up. Let\u2019s try to add some sparsity to make hidden units represent more independent features. The basic way to to it is described by Honglak Lee, and it\u2019s just adding \u03c1\u2212mean(h0) to weights and hidden biases, where \u03c1 is the desired sparsity target, meaning average probability that a certain unit is on. Actually it is possible to add that term only to hidden biases, and we\u2019ll get back to it in a moment.\n",
      "\n",
      "\n",
      "At first I\u2019d like to point out that I haven\u2019t found literally any analysis on the matter. Energy function for Gaussian RBM is just stated (by Hinton, Lee and some other papers I\u2019ve managed to google), and no further comments are made. That\u2019s really disappointing especially because everyone keeps saying Gaussian RBMs are hard to train. And, by the way, this is so true.\n",
      "\n",
      "First, it doesn\u2019t even work at all with uniformly initialized [\u22121m,1m] weights. To make it learn, I had to replace them with normaly distributed weights with zero mean and 0.001 standart deviation (thanks to practical guide again). Any attempt to increate the std value breaks learning like completely.\n",
      "\n",
      "Oh, and I forgot to mention the actual change: for visible units I\u2019ve replaced the activation function with sampling from normal distribution of (hw+bvis) mean and unit variance. To be able to do that I had to rescale the input data to zero mean and unit variance (otherwise it\u2019s also possible to learn precise variance parameters per each unit). Also I guess I can\u2019t use \u201craw\u201d hw+bvis value to collect learning statistic (as I did with Bernoulli probability), so I\u2019m going to sample states everywhere.\n",
      "\n",
      "According to my (kinda sloppy) observations sparsity doesn\u2019t work so good for Gaussian RBM either \u2014 adding sparsity penalty to the weights seems to push the gradient in the wrong direction maybe? Anyway, average hidden activation doesn\u2019t change properly. I\u2019ve followed Lee\u2019s advice about adding sparsity penalty just to visible biases, and now it works better.\n",
      "\n",
      "\n",
      "http://rocknrollnerd.github.io/ml/2015/07/23/finally-rbms.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Testar os dois codigos de sparsos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "    def set_learning_rate(self, epoch, de_con=1e-3):\n",
      "        return self.init_lr / (1. + de_con*epoch)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SGD\n",
      "\n",
      "keras.optimizers.SGD(lr=0.01, momentum=0., decay=0., nesterov=False)\n",
      "\n",
      "Arguments:\n",
      "\n",
      "    lr: float >= 0. Learning rate.\n",
      "    momentum: float >= 0. Parameter updates momentum.\n",
      "    decay: float >= 0. Learning rate decay over each update.\n",
      "    nesterov: boolean. Whether to apply Nesterov momentum.\n",
      "\n",
      "Adagrad\n",
      "\n",
      "keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\n",
      "\n",
      "It is recommended to leave the parameters of this optimizer at their default values.\n",
      "\n",
      "Arguments:\n",
      "\n",
      "    lr: float >= 0. Learning rate.\n",
      "    epsilon: float >= 0.\n",
      "\n",
      "Adadelta\n",
      "\n",
      "keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-6)\n",
      "\n",
      "It is recommended to leave the parameters of this optimizer at their default values.\n",
      "\n",
      "Arguments:\n",
      "\n",
      "    lr: float >= 0. Learning rate. It is recommended to leave it at the default value.\n",
      "    rho: float >= 0.\n",
      "    epsilon: float >= 0. Fuzz factor.\n",
      "\n",
      "For more info, see \"Adadelta: an adaptive learning rate method\" by Matthew Zeiler.\n",
      "RMSprop\n",
      "\n",
      "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
      "\n",
      "It is recommended to leave the parameters of this optimizer at their default values.\n",
      "\n",
      "Arguments:\n",
      "\n",
      "    lr: float >= 0. Learning rate.\n",
      "    rho: float >= 0.\n",
      "    epsilon: float >= 0. Fuzz factor.\n",
      "\n",
      "Adam\n",
      "\n",
      "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
      "\n",
      "Adam optimizer, proposed by Kingma and Lei Ba in Adam: A Method For Stochastic Optimization. Default parameters are those suggested in the paper.\n",
      "\n",
      "Arguments:\n",
      "\n",
      "    lr: float >= 0. Learning rate.\n",
      "    beta_1, beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
      "    epsilon: float >= 0. Fuzz factor.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Available objectives\n",
      "\n",
      "    mean_squared_error / mse\n",
      "    root_mean_squared_error / rmse\n",
      "    mean_absolute_error / mae\n",
      "    mean_absolute_percentage_error / mape\n",
      "    mean_squared_logarithmic_error / msle\n",
      "    squared_hinge\n",
      "    hinge\n",
      "    binary_crossentropy: Also known as logloss.\n",
      "    categorical_crossentropy: Also known as multiclass logloss. Note: using this objective requires that your labels are binary arrays of shape (nb_samples, nb_classes).\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Inicializacao\n",
      "\n",
      "    uniform\n",
      "    lecun_uniform: Uniform initialization scaled by the square root of the number of inputs (LeCun 98).\n",
      "    normal\n",
      "    identity: Use with square 2D layers (shape[0] == shape[1]).\n",
      "    orthogonal: Use with square 2D layers (shape[0] == shape[1]).\n",
      "    zero\n",
      "    glorot_normal: Gaussian initialization scaled by fan_in + fan_out (Glorot 2010)\n",
      "    glorot_uniform\n",
      "    he_normal: Gaussian initialization scaled by fan_in (He et al., 2014)\n",
      "    he_uniform\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}